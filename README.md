# í¬íŠ¸í´ë¦¬ì˜¤ ì„ íƒì„ ìœ„í•œ ì‹¬ì¸µ ê°•í™”í•™ìŠµ (Portfolio Selection via Deep Reinforcement Learning)

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)](https://pytorch.org/)
[![Stable Baselines3](https://img.shields.io/badge/Stable_Baselines3-2.0-red.svg)](https://stable-baselines3.readthedocs.io/en/master/)
[![Gym](https://img.shields.io/badge/Gym-0.26.2-green.svg)](https://gymnasium.farama.org/)

[cite_start][ë°•ì¬ì—°ì˜ ì„ì‚¬ í•™ìœ„ ë…¼ë¬¸ "Portfolio Selection via Deep Reinforcement Learning: Comparative Analysis with Classical Strategies" [cite: 2]ì˜ ê³µì‹ ì½”ë“œ ì €ì¥ì†Œì…ë‹ˆë‹¤.]

[cite_start]ë…¼ë¬¸ ì „ë¬¸ì€ `/thesis` í´ë”ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

---

## ğŸ“œ ê°œìš” (Abstract)

> [cite_start]í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ëŠ” ìœ„í—˜ì„ ìµœì†Œí™”í•˜ë©´ì„œ íˆ¬ì ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìì‚°ì„ ì „ëµì ìœ¼ë¡œ ë°°ë¶„í•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤. [cite: 29] [cite_start]ë³¸ ì—°êµ¬ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„ ìµœì í™”ë¥¼ ìœ„í•´ ì „í†µì ì¸ í‰ê· -ë¶„ì‚° ìµœì í™”(MVO), ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸, ê·¸ë¦¬ê³  ì‹¬ì¸µ ê°•í™”í•™ìŠµ(DRL)ì„ ë¹„êµí•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. [cite: 30] [cite_start]MVOëŠ” ê³µë¶„ì‚° í–‰ë ¬ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ Ledoit-Wolf ì¶•ì†Œ ë°©ë²•ì„ ì‚¬ìš©í•˜ê³  ìµœì í™”ë¥¼ ìœ„í•´ íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ê¸°ë²•ì„ ì ìš©í•©ë‹ˆë‹¤. [cite: 31] [cite_start]ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸ì€ CAPMì—ì„œ íŒŒìƒëœ ì‹œì¥ ê· í˜• ìˆ˜ìµë¥ ì— íˆ¬ììì˜ ê²¬í•´ë¥¼ í†µí•©í•˜ì—¬ MVOë¥¼ í™•ì¥í•˜ë©°, ë” ê· í˜• ì¡íŒ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤. [cite: 32] [cite_start]ë°˜ë©´, DRL ë°©ë²•ìœ¼ë¡œëŠ” PPO(Proximal Policy Optimization)ê°€ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. [cite: 33] [cite_start]ê³¼ê±° ì‹œì¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë°±í…ŒìŠ¤íŒ… ê¸°ë°˜ì˜ ì‹¤ì¦ ë¶„ì„ ê²°ê³¼, DRL ì ‘ê·¼ ë°©ì‹ì´ ëˆ„ì  ìˆ˜ìµë¥ , ì—°ê°„ ìˆ˜ìµë¥ , ë³€ë™ì„±, ìƒ¤í”„ ë¹„ìœ¨ ë“± ë‹¤ì–‘í•œ ì„±ê³¼ ì§€í‘œì—ì„œ MVOì™€ ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸ì„ ëª¨ë‘ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. [cite: 34] [cite_start]ì´ëŸ¬í•œ ê²°ê³¼ëŠ” DRL, íŠ¹íˆ PPO ì „ëµì´ í˜„ëŒ€ ê¸ˆìœµ ì‹œì¥ì—ì„œ ìš°ìˆ˜í•œ ìˆ˜ìµë¥ ì„ ë‹¬ì„±í•˜ê³  ìœ„í—˜ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ë™ì ì´ê³  ì ì‘ì ì¸ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ë¥¼ ìœ„í•œ ê°•ë ¥í•œ ë„êµ¬ë¡œì„œì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤. [cite: 35]

## ğŸš€ í•µì‹¬ ê²°ê³¼ (Key Findings)

ë³¸ ì—°êµ¬ì˜ ì‹¤í—˜ ê²°ê³¼, PPO ê¸°ë°˜ì˜ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ê°€ ê³ ì „ì ì¸ MVO ë° ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸, ê·¸ë¦¬ê³  ë²¤ì¹˜ë§ˆí¬(S&P 500) ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

[cite_start]**ë¯¸êµ­ ì‹œì¥ ì „ëµë³„ ì„±ê³¼ ìš”ì•½ (2012-2023 í‰ê· ) [cite: 659]**

| Metric | PPO Return-LB60 | Mean-Variance (MVO) | Black-Litterman | S&P 500 |
| :--- | :---: | :---: | :---: | :---: |
| **ì—°ê°„ ìˆ˜ìµë¥  (Ann. Return)** | **0.1177** | 0.0612 | 0.1496 | 0.1252 |
| **ì—°ê°„ ë³€ë™ì„± (Ann. Volatility)** | 0.1595 | 0.1773 | 0.1819 | 0.1539 |
| **ìƒ¤í”„ ë¹„ìœ¨ (Sharpe Ratio)** | 0.9733 | 0.6044 | 0.9670 | 1.0912 |
| **ìµœëŒ€ ë‚™í­ (Max Drawdown)** | **0.1169** | 0.1379 | 0.1320 | 0.1182 |
| **ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜** | \$326,126.20 | \$194,396.62 | \$498,448.37 | \$373,500.85 |

[cite_start]*($100,000 ì´ˆê¸° ìë³¸ ê¸°ì¤€ [cite: 596])*

## ğŸ› ï¸ ë¦¬í¬ì§€í† ë¦¬ êµ¬ì¡°

```
/DRL-Portfolio-Optimization
â”œâ”€â”€ /code
â”‚   â”œâ”€â”€ train_ppo_return_reward.py  # PPO ì—ì´ì „íŠ¸ í•™ìŠµ ë° ë°±í…ŒìŠ¤íŒ… ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ requirements.txt            # ì‹¤í–‰ì— í•„ìš”í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬
â”‚
â”œâ”€â”€ /thesis
â”‚   â””â”€â”€ Park_JaeYeon_Masters_Thesis_2024.pdf # ë³¸ í”„ë¡œì íŠ¸ì˜ ì„ì‚¬ í•™ìœ„ ë…¼ë¬¸
â”‚
â”œâ”€â”€ /results
â”‚   â””â”€â”€ (ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ .csv íŒŒì¼ì´ ì´ê³³ì— ì €ì¥ë©ë‹ˆë‹¤)
â”‚
â”œâ”€â”€ .gitignore                      # Gitì´ ì¶”ì í•˜ì§€ ì•Šì„ íŒŒì¼ ëª©ë¡
â””â”€â”€ README.md                       # í”„ë¡œì íŠ¸ ì„¤ëª… íŒŒì¼
```

## ğŸ“ˆ ë°©ë²•ë¡  (Methodology)

### 1. ëª¨ë¸ (Model)
* [cite_start]**PPO (Proximal Policy Optimization)** [cite: 33, 167][cite_start]: `stable-baselines3` ë¼ì´ë¸ŒëŸ¬ë¦¬ [cite: 444]ë¥¼ ì‚¬ìš©í•˜ì—¬ PPO ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

### 2. í™˜ê²½ (Environment)
* [cite_start]**Custom Gym Env**[cite: 447]: `gym` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìƒì†ë°›ì•„ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ì— íŠ¹í™”ëœ ì»¤ìŠ¤í…€ í™˜ê²½ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.
* [cite_start]**State (ìƒíƒœ)**[cite: 394]:
    * [cite_start]`n`ê°œ ìì‚°ì˜ ê³¼ê±° `T`ì¼ê°„(e.g., 60ì¼ [cite: 405][cite_start]) ìˆ˜ìµë¥  [cite: 400]
    * [cite_start]`vol20` (20ì¼ ë¡¤ë§ ë³€ë™ì„±) [cite: 406]
    * [cite_start]`vol20/vol60` (ë‹¨ê¸°/ì¥ê¸° ë³€ë™ì„± ë¹„ìœ¨) [cite: 406]
    * [cite_start]`VIX` (ë¯¸êµ­ ì‹œì¥) [cite: 406] [cite_start]ë˜ëŠ” `VKOSPI` (í•œêµ­ ì‹œì¥) [cite: 406]
* [cite_start]**Action (í–‰ë™)**[cite: 380]:
    * `n-1` ì°¨ì›ì˜ ì—°ì†ì ì¸ ê°’ (0~1)ìœ¼ë¡œ, ê° ìì‚°ì— ëŒ€í•œ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜(ë¹„ì¤‘)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    * [cite_start]ë§ˆì§€ë§‰ ìì‚°(Cash)ì˜ ë¹„ì¤‘ì€ `1 - (ë‚˜ë¨¸ì§€ ìì‚° ë¹„ì¤‘ì˜ í•©)`ìœ¼ë¡œ ìë™ ê³„ì‚°ë©ë‹ˆë‹¤. [cite: 385]
* [cite_start]**Reward (ë³´ìƒ)**:
    * ë³¸ ë¦¬í¬ì§€í† ë¦¬ì˜ ì½”ë“œëŠ” ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì‹¤í—˜ ì¤‘ í•˜ë‚˜ì¸ **ë‹¨ìˆœ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ** (`new_portfolio_value - current_portfolio_value`)ì„ ë³´ìƒ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 3. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ (Training & Testing)
* [cite_start]**Sliding Window (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)**[cite: 459]:
    * [cite_start]ê¸ˆìœµ ë°ì´í„°ì˜ ì‹œê³„ì—´ íŠ¹ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ ë¡¤ë§ ë°©ì‹ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. [cite: 458]
    * [cite_start]**í•™ìŠµ (Train):** 5ë…„ (e.g., 2006-2010) [cite: 460, 462]
    * [cite_start]**ê²€ì¦ (Validation):** 1ë…„ (e.g., 2011) [cite: 460, 464]
    * [cite_start]**í…ŒìŠ¤íŠ¸ (Test):** 1ë…„ (e.g., 2012) [cite: 460, 466]
    * [cite_start]ì´í›„ ìœˆë„ìš°ë¥¼ 1ë…„ì”© ì´ë™(e.g., 2007-2011 í•™ìŠµ -> 2012 ê²€ì¦ -> 2013 í…ŒìŠ¤íŠ¸)í•˜ë©° ì „ì²´ ê¸°ê°„ì„ ë°±í…ŒìŠ¤íŒ…í•©ë‹ˆë‹¤. [cite: 460]
* **ë°ì´í„°:**
    * [cite_start]ë¯¸êµ­ ì‹œì¥ (S&P 500 ì„¹í„° ETF 9ì¢… + í˜„ê¸ˆ) [cite: 594]
    * [cite_start]í•œêµ­ ì‹œì¥ (KOSPI ì£¼ìš” ì¢…ëª© 15ì¢… + í˜„ê¸ˆ) [cite: 711]

## ğŸš€ ì‹¤í–‰ ë°©ë²• (How to Run)

### 1. ì €ì¥ì†Œ ë³µì œ (Clone Repository)
```bash
git clone [https://github.com/](https://github.com/)[Your_Username]/DRL-Portfolio-Optimization.git
cd DRL-Portfolio-Optimization
```

### 2. ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”
```bash
# ê°€ìƒ í™˜ê²½ ìƒì„± (ìµœì´ˆ 1íšŒ)
python -m venv venv

# ê°€ìƒ í™˜ê²½ í™œì„±í™” (Windows)
.\venv\Scripts\activate
# (macOS/Linux)
# source venv/bin/activate
```

### 3. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install -r code/requirements.txt
```

### 4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
# ì½”ë“œ ì‹¤í–‰ (ë¯¸êµ­ ì‹œì¥ ë°ì´í„°ë¡œ í•™ìŠµ ë° ë°±í…ŒìŠ¤íŒ… ì‹œì‘)
python code/train_ppo_return_reward.py
```
* ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ë©´ `models/` í´ë”ì— í•™ìŠµëœ ëª¨ë¸(.zip)ì´ ì €ì¥ë˜ê³ , `results/` í´ë”ì— ìµœì¢… ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ì¸ `PPO_results_1.csv` íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

## ğŸ“ ì €ì (Author)

* **ë°•ì¬ì—° (Jae Yeon Park)** [cite: 3, 8, 15]
* ì„ì‚¬ í•™ìœ„ ë…¼ë¬¸ (2024), Department of Mathematical Sciences [cite: 1, 4, 14]

## ğŸ“œ ë¼ì´ì„ ìŠ¤ (License)

ë³¸ í”„ë¡œì íŠ¸ëŠ” [MIT License](LICENSE) (ë˜ëŠ” ì›í•˜ì‹œëŠ” ë¼ì´ì„ ìŠ¤) í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.