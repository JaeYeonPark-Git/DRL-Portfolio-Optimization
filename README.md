# í¬íŠ¸í´ë¦¬ì˜¤ ì„ íƒì„ ìœ„í•œ ì‹¬ì¸µ ê°•í™”í•™ìŠµ (Portfolio Selection via Deep Reinforcement Learning)

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)](https://pytorch.org/)
[![Stable Baselines3](https://img.shields.io/badge/Stable_Baselines3-2.0-red.svg)](https://stable-baselines3.readthedocs.io/en/master/)
[![Gym](https://img.shields.io/badge/Gym-0.26.2-green.svg)](https://gymnasium.farama.org/)

ë°•ì¬ì—°ì˜ ì„ì‚¬ í•™ìœ„ ë…¼ë¬¸ "Portfolio Selection via Deep Reinforcement Learning: Comparative Analysis with Classical Strategies"ì˜ ê³µì‹ ì½”ë“œ ì €ì¥ì†Œì…ë‹ˆë‹¤.

ë…¼ë¬¸ ì „ë¬¸ì€ `/thesis` í´ë”ì—ì„œ, ì½”ë“œëŠ” `/code` í´ë”ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“œ ê°œìš” (Abstract)

í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ëŠ” ìœ„í—˜ì„ ìµœì†Œí™”í•˜ë©´ì„œ íˆ¬ì ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìì‚°ì„ ì „ëµì ìœ¼ë¡œ ë°°ë¶„í•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„ ìµœì í™”ë¥¼ ìœ„í•´ ì „í†µì ì¸ í‰ê· -ë¶„ì‚° ìµœì í™”(MVO), ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸, ê·¸ë¦¬ê³  ì‹¬ì¸µ ê°•í™”í•™ìŠµ(DRL)ì„ ë¹„êµí•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. MVOëŠ” ê³µë¶„ì‚° í–‰ë ¬ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ Ledoit-Wolf ì¶•ì†Œ ë°©ë²•ì„ ì‚¬ìš©í•˜ê³  ìµœì í™”ë¥¼ ìœ„í•´ íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ê¸°ë²•ì„ ì ìš©í•©ë‹ˆë‹¤. ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸ì€ CAPMì—ì„œ íŒŒìƒëœ ì‹œì¥ ê· í˜• ìˆ˜ìµë¥ ì— íˆ¬ììì˜ ê²¬í•´ë¥¼ í†µí•©í•˜ì—¬ MVOë¥¼ í™•ì¥í•˜ë©°, ë” ê· í˜• ì¡íŒ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤. ë°˜ë©´, DRL ë°©ë²•ìœ¼ë¡œëŠ” PPO(Proximal Policy Optimization)ê°€ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê³¼ê±° ì‹œì¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë°±í…ŒìŠ¤íŒ… ê¸°ë°˜ì˜ ì‹¤ì¦ ë¶„ì„ ê²°ê³¼, DRL ì ‘ê·¼ ë°©ì‹ì´ ëˆ„ì  ìˆ˜ìµë¥ , ì—°ê°„ ìˆ˜ìµë¥ , ë³€ë™ì„±, ìƒ¤í”„ ë¹„ìœ¨ ë“± ë‹¤ì–‘í•œ ì„±ê³¼ ì§€í‘œì—ì„œ MVOì™€ ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸ì„ ëª¨ë‘ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” DRL, íŠ¹íˆ PPO ì „ëµì´ í˜„ëŒ€ ê¸ˆìœµ ì‹œì¥ì—ì„œ ìš°ìˆ˜í•œ ìˆ˜ìµë¥ ì„ ë‹¬ì„±í•˜ê³  ìœ„í—˜ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ë™ì ì´ê³  ì ì‘ì ì¸ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ë¥¼ ìœ„í•œ ê°•ë ¥í•œ ë„êµ¬ë¡œì„œì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“š ë©”ì¸ ì°¸ê³  ë¬¸í—Œ (Main Reference)

ë³¸ í”„ë¡œì íŠ¸ëŠ” J.P. Morgan AI Researchì—ì„œ ë°œí‘œí•œ ë‹¤ìŒ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

* Sood, S., Papasotiriou, K., Vaiciulis, M., & Balch, T. (2023). **[Deep reinforcement learning for optimal portfolio allocation: A comparative study with mean-variance optimization](https://icaps23.icaps-conference.org/papers/finplan/FinPlan23_paper_4.pdf)**. *J.P. Morgan AI Research & Proceedings of the 3rd International Workshop on Financial Planning (FinPlan 2023)*.
  
## ğŸ“œ ê°œìš” (Abstract)

í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ëŠ” ìœ„í—˜ì„ ìµœì†Œí™”í•˜ë©´ì„œ íˆ¬ì ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìì‚°ì„ ì „ëµì ìœ¼ë¡œ ë°°ë¶„í•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„ ìµœì í™”ë¥¼ ìœ„í•´ ì „í†µì ì¸ í‰ê· -ë¶„ì‚° ìµœì í™”(MVO), ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸, ê·¸ë¦¬ê³  ì‹¬ì¸µ ê°•í™”í•™ìŠµ(DRL)ì„ ë¹„êµí•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. MVOëŠ” ê³µë¶„ì‚° í–‰ë ¬ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ Ledoit-Wolf ì¶•ì†Œ ë°©ë²•ì„ ì‚¬ìš©í•˜ê³  ìµœì í™”ë¥¼ ìœ„í•´ íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ê¸°ë²•ì„ ì ìš©í•©ë‹ˆë‹¤. ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸ì€ CAPMì—ì„œ íŒŒìƒëœ ì‹œì¥ ê· í˜• ìˆ˜ìµë¥ ì— íˆ¬ììì˜ ê²¬í•´ë¥¼ í†µí•©í•˜ì—¬ MVOë¥¼ í™•ì¥í•˜ë©°, ë” ê· í˜• ì¡íŒ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤. ë°˜ë©´, DRL ë°©ë²•ìœ¼ë¡œëŠ” PPO(Proximal Policy Optimization)ê°€ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê³¼ê±° ì‹œì¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë°±í…ŒìŠ¤íŒ… ê¸°ë°˜ì˜ ì‹¤ì¦ ë¶„ì„ ê²°ê³¼, DRL ì ‘ê·¼ ë°©ì‹ì´ ëˆ„ì  ìˆ˜ìµë¥ , ì—°ê°„ ìˆ˜ìµë¥ , ë³€ë™ì„±, ìƒ¤í”„ ë¹„ìœ¨ ë“± ë‹¤ì–‘í•œ ì„±ê³¼ ì§€í‘œì—ì„œ MVOì™€ ë¸”ë™-ë¦¬í„°ë§Œ ëª¨ë¸ì„ ëª¨ë‘ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

**ë³¸ ì—°êµ¬ëŠ” S. Sood ë“±ì˜ J.P. Morgan AI Research ë…¼ë¬¸ "Deep reinforcement learning for optimal portfolio allocation"ì„ ë©”ì¸ ë ˆí¼ëŸ°ìŠ¤ë¡œ í•˜ì—¬, ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ì¬í˜„í•˜ê³  í•œêµ­ ë° ë¯¸êµ­ ì‹œì¥ ë°ì´í„°ì— ì ìš©í•˜ì—¬ ë¹„êµ ë¶„ì„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.**

## ğŸ“š ë©”ì¸ ì°¸ê³  ë¬¸í—Œ (Main Reference)

ë³¸ í”„ë¡œì íŠ¸ëŠ” J.P. Morgan AI Researchì—ì„œ ë°œí‘œí•œ ë‹¤ìŒ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

* Sood, S., Papasotiriou, K., Vaiciulis, M., & Balch, T. (2023). **[Deep reinforcement learning for optimal portfolio allocation: A comparative study with mean-variance optimization](https://icaps23.icaps-conference.org/papers/finplan/FinPlan23_paper_4.pdf)**. *J.P. Morgan AI Research & Proceedings of the 3rd International Workshop on Financial Planning (FinPlan 2023)*.

---

## ğŸš€ í•µì‹¬ ê²°ê³¼ (Key Findings)

### ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì‹œì¥ (U.S. Market)

**ì „ëµë³„ ì„±ê³¼ ìš”ì•½ (2012-2023 í‰ê· )**

| Metric | PPO Return-LB60 | Mean-Variance (MVO) | Black-Litterman | S&P 500 |
| :--- | :---: | :---: | :---: | :---: |
| **ì—°ê°„ ìˆ˜ìµë¥  (Ann. Return)** | **0.1177** | 0.0612 | 0.1496 | 0.1252 |
| **ì—°ê°„ ë³€ë™ì„± (Ann. Volatility)** | 0.1595 | 0.1773 | 0.1819 | 0.1539 |
| **ìƒ¤í”„ ë¹„ìœ¨ (Sharpe Ratio)** | 0.9733 | 0.6044 | 0.9670 | 1.0912 |
| **ìµœëŒ€ ë‚™í­ (Max Drawdown)** | **0.1169** | 0.1379 | 0.1320 | 0.1182 |
| **ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜** | \$326,126.20 | \$194,396.62 | \$498,448.37 | \$373,500.85 |

*($100,000 ì´ˆê¸° ìë³¸ ê¸°ì¤€)*

**ì—°ê°„ ìˆ˜ìµë¥  (Annualized Return) - U.S. Market**
![U.S. Market Annualized Return](./assets/us_annualized_return.png)

**ìƒ¤í”„ ë¹„ìœ¨ (Sharpe Ratio) - U.S. Market**
![U.S. Market Sharpe Ratio](./assets/us_sharpe_ratio.png)

### ğŸ‡°ğŸ‡· í•œêµ­ ì‹œì¥ (South Korean Market)

**ì „ëµë³„ ì„±ê³¼ ìš”ì•½ (2012-2023 í‰ê· )**

| Metric | PPO Lookback 60 | Mean-Variance (MVO) | Black-Litterman | KOSPI |
| :--- | :---: | :---: | :---: | :---: |
| **ì—°ê°„ ìˆ˜ìµë¥  (Ann. Return)** | **0.1434** | 0.0996 | 0.0583 | 0.0463 |
| **ì—°ê°„ ë³€ë™ì„± (Ann. Volatility)** | 0.2151 | 0.2571 | 0.2612 | 0.1483 |
| **ìƒ¤í”„ ë¹„ìœ¨ (Sharpe Ratio)** | **0.5456** | 0.3021 | 0.1536 | 0.3072 |
| **ìµœëŒ€ ë‚™í­ (Max Drawdown)** | 0.1849 | 0.2261 | 0.2418 | 0.1533 |
| **ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜** | **â‚©372,696,087** | â‚©197,822,628 | â‚©122,265,282 | â‚©135,827,664 |

*(100,000,000 KRW ì´ˆê¸° ìë³¸ ê¸°ì¤€)*

**ì—°ê°„ ìˆ˜ìµë¥  (Annualized Return) - South Korean Market**
![South Korean Market Annualized Return](./assets/kor_annualized_return.png)

**ìƒ¤í”„ ë¹„ìœ¨ (Sharpe Ratio) - South Korean Market**
![South Korean Market Sharpe Ratio](./assets/kor_sharpe_ratio.png)

---

## ğŸ› ï¸ ë¦¬í¬ì§€í† ë¦¬ êµ¬ì¡°

```
/DRL-Portfolio-Optimization
â”œâ”€â”€ /code
â”‚   â”œâ”€â”€ train_ppo_return_reward.py  # PPO ì—ì´ì „íŠ¸ í•™ìŠµ ë° ë°±í…ŒìŠ¤íŒ… ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ requirements.txt            # ì‹¤í–‰ì— í•„ìš”í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬
â”‚
â”œâ”€â”€ /thesis
â”‚   â””â”€â”€ Park_JaeYeon_Masters_Thesis_2024.pdf # ë³¸ í”„ë¡œì íŠ¸ì˜ ì„ì‚¬ í•™ìœ„ ë…¼ë¬¸
â”‚
â”œâ”€â”€ /results
â”‚   â””â”€â”€ (ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ .csv íŒŒì¼ì´ ì´ê³³ì— ì €ì¥ë©ë‹ˆë‹¤)
â”‚
â”œâ”€â”€ .gitignore                      # Gitì´ ì¶”ì í•˜ì§€ ì•Šì„ íŒŒì¼ ëª©ë¡
â””â”€â”€ README.md                       # í”„ë¡œì íŠ¸ ì„¤ëª… íŒŒì¼
```

## ğŸ“ˆ ë°©ë²•ë¡  (Methodology)

### 1. ëª¨ë¸ (Model)
* **PPO (Proximal Policy Optimization)**: `stable-baselines3` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ PPO ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

### 2. í™˜ê²½ (Environment)
* **Custom Gym Env**: `gym` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìƒì†ë°›ì•„ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ì— íŠ¹í™”ëœ ì»¤ìŠ¤í…€ í™˜ê²½ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.
* **State (ìƒíƒœ)**:
    * `n`ê°œ ìì‚°ì˜ ê³¼ê±° `T`ì¼ê°„(e.g., 60ì¼) ìˆ˜ìµë¥ 
    * `vol20` (20ì¼ ë¡¤ë§ ë³€ë™ì„±)
    * `vol20/vol60` (ë‹¨ê¸°/ì¥ê¸° ë³€ë™ì„± ë¹„ìœ¨)
    * `VIX` (ë¯¸êµ­ ì‹œì¥) ë˜ëŠ” `VKOSPI` (í•œêµ­ ì‹œì¥)
* **Action (í–‰ë™)**:
    * `n-1` ì°¨ì›ì˜ ì—°ì†ì ì¸ ê°’ (0~1)ìœ¼ë¡œ, ê° ìì‚°ì— ëŒ€í•œ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜(ë¹„ì¤‘)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    * ë§ˆì§€ë§‰ ìì‚°(Cash)ì˜ ë¹„ì¤‘ì€ `1 - (ë‚˜ë¨¸ì§€ ìì‚° ë¹„ì¤‘ì˜ í•©)`ìœ¼ë¡œ ìë™ ê³„ì‚°ë©ë‹ˆë‹¤.
* **Reward (ë³´ìƒ)**:
    * ë³¸ ë¦¬í¬ì§€í† ë¦¬ì˜ ì½”ë“œëŠ” ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì‹¤í—˜ ì¤‘ í•˜ë‚˜ì¸ **ë‹¨ìˆœ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ** (`new_portfolio_value - current_portfolio_value`)ì„ ë³´ìƒ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 3. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ (Training & Testing)
* **Sliding Window (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)**:
    * ê¸ˆìœµ ë°ì´í„°ì˜ ì‹œê³„ì—´ íŠ¹ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ ë¡¤ë§ ë°©ì‹ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
    * **í•™ìŠµ (Train):** 5ë…„ (e.g., 2006-2010)
    * **ê²€ì¦ (Validation):** 1ë…„ (e.g., 2011)
    * **í…ŒìŠ¤íŠ¸ (Test):** 1ë…„ (e.g., 2012)
    * ì´í›„ ìœˆë„ìš°ë¥¼ 1ë…„ì”© ì´ë™(e.g., 2007-2011 í•™ìŠµ -> 2012 ê²€ì¦ -> 2013 í…ŒìŠ¤íŠ¸)í•˜ë©° ì „ì²´ ê¸°ê°„ì„ ë°±í…ŒìŠ¤íŒ…í•©ë‹ˆë‹¤.
* **ë°ì´í„°:**
    * ë¯¸êµ­ ì‹œì¥ (S&P 500 ì„¹í„° ETF 9ì¢… + í˜„ê¸ˆ)
    * í•œêµ­ ì‹œì¥ (KOSPI ì£¼ìš” ì¢…ëª© 15ì¢… + í˜„ê¸ˆ)

## ğŸš€ ì‹¤í–‰ ë°©ë²• (How to Run)

### 1. ì €ì¥ì†Œ ë³µì œ (Clone Repository)
```bash
git clone [https://github.com/](https://github.com/)[Your_Username]/DRL-Portfolio-Optimization.git
cd DRL-Portfolio-Optimization
```

### 2. ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”
```bash
# ê°€ìƒ í™˜ê²½ ìƒì„± (ìµœì´ˆ 1íšŒ)
python -m venv venv

# ê°€ìƒ í™˜ê²½ í™œì„±í™” (Windows)
.\venv\Scripts\activate
# (macOS/Linux)
# source venv/bin/activate
```

### 3. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install -r code/requirements.txt
```

### 4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
# ì½”ë“œ ì‹¤í–‰ (ë¯¸êµ­ ì‹œì¥ ë°ì´í„°ë¡œ í•™ìŠµ ë° ë°±í…ŒìŠ¤íŒ… ì‹œì‘)
python code/train_ppo_return_reward.py
```
* ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ë©´ `models/` í´ë”ì— í•™ìŠµëœ ëª¨ë¸(.zip)ì´ ì €ì¥ë˜ê³ , `results/` í´ë”ì— ìµœì¢… ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ì¸ `PPO_results_1.csv` íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

## ğŸ“ ì €ì (Author)

* **ë°•ì¬ì—° (Jae Yeon Park)**
* Portfolio Selection via Deep Reinforcement Learning: Comparative Analysis with Classical Strategies (2024), Department of Mathematical Sciences, UNIST


